{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from pinyin import get as pinyinget\n",
    "# from hanzipy.dictionary import HanziDictionary\n",
    "\n",
    "# searcher = HanziDictionary()\n",
    "# result = searcher.definition_lookup(\"好\", script_type=\"simplified\")\n",
    "# print(result)\n",
    "\n",
    "# Initialize the synonym dictionary with defaultdict\n",
    "thesaurus_dict = defaultdict(lambda: {\"SynonymSet\": [], \"RelatedSet\": [], \"IndependentSet\": [], \"AntonymSet\": [], \"NegationSet\": []})\n",
    "\n",
    "# Path to the uploaded file\n",
    "\n",
    "MAX_ITEMS = 100000 # For debug purposes\n",
    "\n",
    "BIG_ITEMS = 50 # For debug purposes\n",
    "\n",
    "PC_NEWLINE = chr(0xEAB1)\n",
    "PC_RIGHT_TRIANGLE = \"»\"  # ▸\n",
    "\n",
    "def pleco_make_bold(text):\n",
    "    return f\"{chr(0xEAB2)}{text}{chr(0xEAB3)}\"\n",
    "\n",
    "def pleco_make_link(text):\n",
    "    return f\"{chr(0xEAB8)}{text}{chr(0xEABB)}\"\n",
    "\n",
    "def make_linked_items(cur_item, list_items):\n",
    "    items = sorted(list(set(list_items))) # Removes duplicated lines \n",
    "    contents = \"\"\n",
    "\n",
    "    for line in items:\n",
    "        tokens = set(line.split(' '))\n",
    "        tokens.discard(cur_item)\n",
    "        \n",
    "        words = [(pleco_make_link(word) + \" \" + pinyinget(word)) for word in sorted(list(tokens))]\n",
    "        contents += f\"{PC_RIGHT_TRIANGLE} {\" \".join(words)}\\n\"\n",
    "    \n",
    "    return contents\n",
    "\n",
    "import json\n",
    "\n",
    "thesaurus_dict = defaultdict(lambda: {\"SynonymSet\": [], \"RelatedSet\": [], \"IndependentSet\": [], \"AntonymSet\": [], \"NegationSet\": []})\n",
    "\n",
    "sym_list_path = \"output_list.json\"\n",
    "sym_details_path = \"output_details.json\"\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(sym_list_path, 'r', encoding='utf-8') as file:\n",
    "    items = json.load(file)\n",
    "\n",
    "    for item in items:\n",
    "        word = item['Word']\n",
    "        syns_ = item['Synonyms'].split('、')\n",
    "        syns_set = set(syns)\n",
    "        syns_set.add(word)\n",
    "        \n",
    "        word_list = words.split()\n",
    "\n",
    "        # Populate the dictionary\n",
    "        for word in word_list:\n",
    "            thesaurus_dict[word][thesaurus_type].append(words)\n",
    "\n",
    "        pass\n",
    "\n",
    "# Print the loaded data\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tudien",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
